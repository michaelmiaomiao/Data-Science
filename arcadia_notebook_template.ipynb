{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Name\n",
    "\n",
    "- **Author**: your name here\n",
    "- **Created on**: today's date\n",
    "- **Service account**: `sa-uber`\n",
    "\n",
    "**Description**: what does this notebook do?\n",
    "\n",
    "**Additional Documentation**:\n",
    "- Link to additional documentation (if any)\n",
    "\n",
    "## Methodology\n",
    "\n",
    "1. Pull data.\n",
    "2. Do stuff.\n",
    "3. Save table.\n",
    "4. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "script_start_ts = time.time()  # Begin script stopwatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Configurations\n",
    "\n",
    "Define variables that require user configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "script_name = 'NOTEBOOK_NAME'\n",
    "initials = 'ABC'\n",
    "spark_config_size = 'small'  # One of 'small', 'medium', 'large' or 'extra_large'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ai.datetools\n",
    "end_dt = ai.datetools.date()\n",
    "start_dt = end_dt - 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These timestamps are in epoch-milliseconds format\n",
    "start_ts = ai.datetools.datetime(start_dt).hql\n",
    "end_ts = ai.datetools.datetime(end_dt).hql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "\n",
    "Import libraries for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import socket\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "# Pyspark imports\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "# AI/Athena imports\n",
    "from athena.pyspark_utils import SparkSessionManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options\n",
    "\n",
    "Set global options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 2000)\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Setup\n",
    "\n",
    "Instantiate Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_spark_app_name(initials, script_name):\n",
    "    \"\"\"\n",
    "    Build Spark application name string.\n",
    "    \"\"\"\n",
    "    return '_'.join([initials.lower(),\n",
    "                     script_name.lower().replace(' ', '_'),\n",
    "                     str(int(time.time())),\n",
    "                     socket.gethostname()])\n",
    "\n",
    "\n",
    "def define_spark_config(app_name, config_size):\n",
    "    \"\"\"\n",
    "    Get a dictionary of Spark configurations.\n",
    "    \"\"\"\n",
    "    config_options = {\n",
    "        'small': {\n",
    "            'spark.executor.instances': 10,\n",
    "            'spark.executor.cores': 2,\n",
    "            'spark.executor.memory': '5g',\n",
    "            'spark.executor.memoryOverhead': '1g',\n",
    "        },\n",
    "        'medium': {\n",
    "            'spark.executor.instances': 14,\n",
    "            'spark.executor.cores': 4,\n",
    "            'spark.executor.memory': '8g',\n",
    "            'spark.executor.memoryOverhead': '2g',\n",
    "        },\n",
    "        'large': {\n",
    "            'spark.executor.instances': 20,\n",
    "            'spark.executor.cores': 4,\n",
    "            'spark.executor.memory': '12g',\n",
    "            'spark.executor.memoryOverhead': '2g',\n",
    "        },\n",
    "        'extra_large': {\n",
    "            'spark.executor.memory': '24g',\n",
    "            'spark.executor.instances': 20,\n",
    "            'spark.executor.cores': 4,\n",
    "            'spark.driver.memory': '16g',\n",
    "            'spark.driver.maxResultSize': '8g',\n",
    "            'spark.sql.shuffle.partitions': 80,\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    config = {\n",
    "        'spark.driver.memory': '5g',\n",
    "        'spark.driver.memoryOverhead': '2g',\n",
    "        'spark.task.cpus': 1,\n",
    "        'spark.sql.shuffle.partitions': '50',\n",
    "    }\n",
    "    \n",
    "    config.update(config_options[config_size])\n",
    "    return config\n",
    "\n",
    "\n",
    "def is_spark_active(spark):\n",
    "    \"\"\"\n",
    "    Return the active status of a SparkSession object as a boolean.\n",
    "    \"\"\"\n",
    "    return not spark._jsc.sc().isStopped()\n",
    "\n",
    "\n",
    "def initialize_spark(initials, script_name, config_size):\n",
    "    \"\"\"\n",
    "    Initialize a Spark session and return the SparkSession object. If\n",
    "    there's already a Spark session in that global environment that's\n",
    "    active, return that object instead.\n",
    "    \"\"\"\n",
    "    app_name = build_spark_app_name(initials, script_name)\n",
    "\n",
    "    if 'spark' in globals():\n",
    "        if is_spark_active(spark):\n",
    "            print('Active, global SparkSession object `spark` already initialized')\n",
    "            return spark\n",
    "\n",
    "    config = define_spark_config(app_name, config_size)\n",
    "    return SparkSessionManager.initialize_session(application_name=app_name, conf_dict=config)\n",
    "\n",
    "\n",
    "spark = initialize_spark(initials, script_name, spark_config_size)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "Configure default plotting behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def awesome_settings():\n",
    "    \"\"\"\n",
    "    Configure default plotting settings.\n",
    "    \"\"\"\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    sns.set_style('whitegrid')    \n",
    "    sns.set_context('paper', font_scale=2.5)\n",
    "    sns.set_palette('hls')\n",
    "\n",
    "    plt.rcParams['figure.figsize'] = (20.0, 8.0)\n",
    "    plt.rcParams['savefig.dpi'] = 300\n",
    "    plt.rcParams['lines.linewidth'] = 2\n",
    "    plt.rcParams['legend.fancybox'] = True\n",
    "    plt.rcParams['legend.shadow'] = True\n",
    "    plt.rcParams['font.family'] = 'sans-serif'\n",
    "    plt.rcParams['figure.autolayout'] = True\n",
    "\n",
    "    \n",
    "awesome_settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verbosity\n",
    "\n",
    "Configure verbose behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Stopwatch(object):\n",
    "    \"\"\"\n",
    "    Manage script runtime statistics.\n",
    "    \"\"\"\n",
    "    def __init__(self, script_start_ts):\n",
    "        self.script_start_ts = script_start_ts  # Should be the result of time.time()\n",
    "    \n",
    "    def lap(self, msg=None):\n",
    "        \"\"\"\n",
    "        Print an optional message with a timestamp and time elapsed since script start.\n",
    "        \"\"\"\n",
    "        import re\n",
    "        import time\n",
    "        from datetime import datetime\n",
    "        from pytz import reference\n",
    "\n",
    "        now = datetime.now()\n",
    "        now_ts = time.time()\n",
    "        \n",
    "        localtime = reference.LocalTimezone()\n",
    "        now_str = now.strftime('%a, %d-%b-%Y %H:%M:%S, ' + localtime.tzname(now))\n",
    "        \n",
    "        elapsed_minutes = round((now_ts - self.script_start_ts) / 60, 2)\n",
    "    \n",
    "        msg_str = '{msg} at '.format(msg=msg) if isinstance(msg, str) else 'Lap at '\n",
    "        \n",
    "        stopwatch_str = '{msg_str}{now_str}, {elapsed_minutes} minutes since script start time'.format(**locals())\n",
    "        stopwatch_str = re.sub(r'\\s+', ' ', stopwatch_str).strip()\n",
    "        print(stopwatch_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwatch = Stopwatch(script_start_ts)\n",
    "stopwatch.lap('Setup completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Operational Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# X. Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Spark Session\n",
    "\n",
    "Release our resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Script Stopwatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwatch.lap('Completed successfully')"
   ]
  }
 ],
 "metadata": {
  "athena": {
   "image_tag": "python3",
   "name": "Template",
   "owner": "andonis",
   "service_account": "sa-uber",
   "servicedef_version": 30004
  },
  "athena_version": 10001,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}